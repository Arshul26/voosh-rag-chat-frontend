// import axios from 'axios';
// import { io } from 'socket.io-client';

// const BASE_URL = 'http://localhost:4000';

// export const fetchChatReply = async (sessionId, message) => {
//   const response = await axios.post(`${BASE_URL}/api/chat`, { sessionId, message });
//   return response.data;
// };

// export const fetchSessionHistory = async (sessionId) => {
//   const response = await axios.get(`${BASE_URL}/api/session/${sessionId}/history`);
//   return response.data.history;
// };

// export const resetSession = async (sessionId) => {
//   const response = await axios.post(`${BASE_URL}/api/session/${sessionId}/reset`);
//   return response.data;
// };

// // WebSocket connection
// export const socket = io(BASE_URL);


// /src/services/api.js

// Mock socket object to prevent runtime errors
export const socket = {
  on: () => {},
  off: () => {},
  emit: () => {},
};

// Mock fetchChatReply to show static message instead of calling backend
export const fetchChatReply = async (sessionId, message) => {
  return {
    text: "âš ï¸ Hey! VooshFoods Technologies: My Chatbot is working fine as per the assignment, " +
          "but I cannot make the LLM adapters live due to heavy RAM usage of sentence-transformers. " +
          "Free tiers (Render, Railway) cannot handle it. Please check the demo video or request a screen share. " +
          "I dont have credit card to pay for the billing of render of railway. " +
          "I am using the RAG pipeline fully, but live inference is disabled.",
  };
};

// Mock fetchSessionHistory to return empty array (no real backend)
export const fetchSessionHistory = async (sessionId) => {
  return [];
};

// Mock resetSession (does nothing)
export const resetSession = async (sessionId) => {
  return { status: "ok" };
};
